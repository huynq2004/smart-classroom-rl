# ğŸ“˜ Smart Room RL â€“ Reinforcement Learning cho PhÃ²ng Há»c ThÃ´ng Minh

## 1. Giá»›i thiá»‡u
Dá»± Ã¡n nÃ y mÃ´ phá»ng vÃ  huáº¥n luyá»‡n cÃ¡c thuáº­t toÃ¡n **há»c tÄƒng cÆ°á»ng (RL)** Ä‘á»ƒ Ä‘iá»u khiá»ƒn **nhiá»u thiáº¿t bá»‹ trong phÃ²ng há»c thÃ´ng minh** (Ä‘Ã¨n, quáº¡t, Ä‘iá»u hÃ²a, rÃ¨m cá»­a, â€¦).  
Má»¥c tiÃªu: cÃ¢n báº±ng **thoáº£i mÃ¡i (comfort)** vÃ  **tiáº¿t kiá»‡m nÄƒng lÆ°á»£ng (energy saving)**, Ä‘á»“ng thá»i giáº£m **switching cost** (sá»‘ láº§n báº­t/táº¯t thiáº¿t bá»‹).

- **MÃ´i trÆ°á»ng (Environment):** tráº¡ng thÃ¡i liÃªn tá»¥c (nhiá»‡t Ä‘á»™, Ä‘á»™ sÃ¡ng, sá»‘ ngÆ°á»i, nhiá»‡t Ä‘á»™ ngoÃ i trá»i, tráº¡ng thÃ¡i thiáº¿t bá»‹).  
- **HÃ nh Ä‘á»™ng (Action):** rá»i ráº¡c Ä‘a chiá»u (multi-discrete), vÃ­ dá»¥: quáº¡t {0,1,2}, Ä‘iá»u hÃ²a {0,1,2}, Ä‘Ã¨n {0,1}.  
- **Reward:** káº¿t há»£p ba yáº¿u tá»‘ (nÄƒng lÆ°á»£ng, comfort, switching cost).  

## 2. Thuáº­t toÃ¡n triá»ƒn khai
Trong dá»± Ã¡n, nhÃ³m thá»­ nghiá»‡m **3 thuáº­t toÃ¡n**:
1. **BDQ-lite (Factored Double Dueling DQN)** â€“ tÃ¡ch hÃ nh Ä‘á»™ng thÃ nh nhiá»u nhÃ¡nh, phÃ¹ há»£p vá»›i action space lá»›n.  
2. **Multi-head DQN** â€“ baseline, má»—i head dá»± Ä‘oÃ¡n má»™t nhÃ¡nh hÃ nh Ä‘á»™ng.  
3. **Auto-Regressive Q** â€“ chá»n hÃ nh Ä‘á»™ng tuáº§n tá»± theo tá»«ng thiáº¿t bá»‹.  

## 3. Cáº¥u trÃºc dá»± Ã¡n

```plaintext
smart-room-rl/
â”‚â”€â”€ README.md               # TÃ i liá»‡u mÃ´ táº£ (file nÃ y)
â”‚â”€â”€ requirements.txt        # ThÆ° viá»‡n cáº§n cÃ i (torch, gymnasium, numpy,â€¦)
â”‚â”€â”€ main.py                 # Äiá»ƒm khá»Ÿi cháº¡y, gá»i thá»­ nghiá»‡m
â”‚â”€â”€ config.py               # Tham sá»‘ chung (gamma, epsilon, batch_size, â€¦)

â”œâ”€â”€ envs/                   # MÃ´i trÆ°á»ng phÃ²ng há»c
â”‚   â”œâ”€â”€ smart_room_env.py   # Äá»‹nh nghÄ©a MDP: state, action, reward, step
â”‚   â””â”€â”€ utils.py            # HÃ m phá»¥ (tÃ­nh comfort, energy, lux,...)

â”œâ”€â”€ algorithms/             # CÃ i Ä‘áº·t cÃ¡c thuáº­t toÃ¡n
â”‚   â”œâ”€â”€ bdq_lite.py         # BDQ-lite agent
â”‚   â”œâ”€â”€ multi_head_dqn.py   # Multi-head DQN agent
â”‚   â”œâ”€â”€ ar_q.py             # Auto-Regressive Q agent
â”‚   â””â”€â”€ common/
â”‚       â”œâ”€â”€ networks.py     # Kiáº¿n trÃºc máº¡ng (trunk, dueling head, multi-head)
â”‚       â”œâ”€â”€ replay_buffer.py# Bá»™ nhá»› lÆ°u kinh nghiá»‡m
â”‚       â””â”€â”€ utils.py        # Soft update target net, epsilon decay

â”œâ”€â”€ experiments/            # ThÃ­ nghiá»‡m
â”‚   â”œâ”€â”€ run_bdq.py          # Script huáº¥n luyá»‡n BDQ-lite
â”‚   â”œâ”€â”€ run_multi_head.py   # Script huáº¥n luyá»‡n Multi-head DQN
â”‚   â”œâ”€â”€ run_arq.py          # Script huáº¥n luyá»‡n Auto-Regressive Q
â”‚   â””â”€â”€ configs/            # Hyperparams cho tá»«ng thuáº­t toÃ¡n (.json/.yaml)

â”œâ”€â”€ results/                # Káº¿t quáº£ huáº¥n luyá»‡n
â”‚   â”œâ”€â”€ logs/               # Reward theo episode, loss
â”‚   â”œâ”€â”€ models/             # Trá»ng sá»‘ máº¡ng (Î¸, Î¸_target) Ä‘Ã£ train
â”‚   â””â”€â”€ figures/            # Biá»ƒu Ä‘á»“ reward, comfort, energy

â””â”€â”€ report/                 # BÃ¡o cÃ¡o + slide nhÃ³m
    â”œâ”€â”€ report.docx
    â”œâ”€â”€ presentation.pptx
    â””â”€â”€ images/             # SÆ¡ Ä‘á»“, hÃ¬nh minh há»a
```

## 4. Luá»“ng hoáº¡t Ä‘á»™ng
1. **Cháº¡y thÃ­ nghiá»‡m:**  
   ```bash
   python experiments/run_bdq.py --config experiments/configs/bdq.json
   ```
   â†’ Khá»Ÿi táº¡o mÃ´i trÆ°á»ng + agent BDQ â†’ train qua nhiá»u episodes.

2. **Má»—i episode:**  
   - `env.reset()` â†’ tráº¡ng thÃ¡i ban Ä‘áº§u.  
   - Agent chá»n action (online Q hoáº·c random).  
   - `env.step(action)` â†’ tráº¡ng thÃ¡i má»›i + reward.  
   - Transition lÆ°u vÃ o ReplayBuffer.  
   - Agent update máº¡ng Q (tÃ­nh Q_pred, Q_target).  

3. **Sau huáº¥n luyá»‡n:**  
   - Káº¿t quáº£ (reward curve, energy/comfort metrics) Ä‘Æ°á»£c lÆ°u trong `results/figures/`.  
   - Model Ä‘Ã£ train lÆ°u trong `results/models/`.  
   - Biá»ƒu Ä‘á»“ Ä‘Æ°a vÃ o `report/presentation.pptx`.  
 
